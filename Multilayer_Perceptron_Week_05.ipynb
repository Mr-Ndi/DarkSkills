{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LssVahhK90Rn",
        "mbhTYRu4HZNf",
        "m5Mb-7D2KSeU",
        "ZGjzutLmKYWT"
      ],
      "authorship_tag": "ABX9TyMdagXADM1E8sT9RL5DRngl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mr-Ndi/DarkSkills/blob/main/Multilayer_Perceptron_Week_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training an MLP on MNIST with NumPy"
      ],
      "metadata": {
        "id": "TljF7zpM9Wd0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section1 Train-Validation-Test"
      ],
      "metadata": {
        "id": "LssVahhK90Rn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  2.Load and Prepare the Dataset"
      ],
      "metadata": {
        "id": "g_xB-bIq-Lph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "\n",
        "X = X.values # This returns the array values\n",
        "y = y.astype(int).values # This returns the array values"
      ],
      "metadata": {
        "id": "FT09JTTO94lZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing: Normalize the data\n"
      ],
      "metadata": {
        "id": "TkM_xe5K-SMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = ((X / 255.) - .5) * 2"
      ],
      "metadata": {
        "id": "UBtBjU8p-Veq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize sample digits"
      ],
      "metadata": {
        "id": "69nqlGbd-Wi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\n",
        "\n",
        "ax = ax.flatten()\n",
        "for i in range(10):\n",
        "    img = X[y == i][0].reshape(28, 28)\n",
        "    ax[i].imshow(img, cmap='Greys')\n",
        "ax[0].set_xticks([])\n",
        "ax[0].set_yticks([])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "nv8rTvTD-ZgS",
        "outputId": "ad236d91-6f8b-40c4-9832-b48d166bffc3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Train-Validation-Test Split"
      ],
      "metadata": {
        "id": "Jw1kXmny-7PU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-YYBcaJwHgW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Splits:\n",
        "# - 60k train+valid, 10k test\n",
        "# - 55k train, 5k valid\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000, random_state=123)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=5000, random_state=123)"
      ],
      "metadata": {
        "id": "fVohvEuNE84B"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Create Mini-batches"
      ],
      "metadata": {
        "id": "mbhTYRu4HZNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def minibatch_generator(X, y, minibatch_size):\n",
        "    indices = np.arange(X.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    for start_idx in range(0, indices.shape[0] - minibatch_size + 1, minibatch_size):\n",
        "        batch_idx = indices[start_idx:start_idx + minibatch_size]\n",
        "        yield X[batch_idx], y[batch_idx]"
      ],
      "metadata": {
        "id": "PEIgoqISHm2T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Build Core Components of MLP"
      ],
      "metadata": {
        "id": "hED0xHOzIMnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###A. Linear Layer (Fully Connected Layer)"
      ],
      "metadata": {
        "id": "m5Mb-7D2KSeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We are going to\n",
        "  # Initialize weight with small random values\n",
        "  # Initialize bias with zeros\n",
        "  # Initialize grad_weight with zeros. Same shape as weight\n",
        "  # Initialize grad_bias with zeros. Same shape as bias\n",
        "  # Implement forward()\n",
        "\n",
        "class Linear:\n",
        "    def __init__(self, in_features, out_features):\n",
        "        # Initialize weight to small random values (normally distributed)\n",
        "        self.weight = np.random.randn(in_features, out_features) * 0.01\n",
        "\n",
        "        # Initialize bias to zeros\n",
        "        self.bias = np.zeros((1, out_features))\n",
        "\n",
        "        # Initialize gradients for weight and bias to zeros\n",
        "        self.grad_weight = np.zeros_like(self.weight)\n",
        "        self.grad_bias = np.zeros_like(self.bias)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.input = x  # Keep input for backward pass\n",
        "        output = np.dot(x, self.weight) + self.bias  # Linear transformation\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        # Compute the gradient of the input\n",
        "        grad_input = np.dot(grad_output, self.weight.T)\n",
        "\n",
        "        # Compute gradients for weight and bias\n",
        "        self.grad_weight[...] = np.dot(self.input.T, grad_output)  # Weight gradient\n",
        "        self.grad_bias[...] = np.mean(grad_output, axis=0, keepdims=True)  # Bias gradient\n",
        "\n",
        "        return grad_input\n",
        "\n",
        "    def parameters(self):\n",
        "        \"\"\"Returns the parameters (weights and bias) of this layer.\"\"\"\n",
        "        return [self.weight, self.bias]\n",
        "\n",
        "    def gradients(self):\n",
        "        \"\"\"Returns the gradients (weight and bias gradients) of this layer.\"\"\"\n",
        "        return [self.grad_weight, self.grad_bias]\n"
      ],
      "metadata": {
        "id": "d1Qam9asIP8H"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###B. Sigmoid Activation Function"
      ],
      "metadata": {
        "id": "ZGjzutLmKYWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we are going to Implement the sigmoid function in forward\n",
        "class Sigmoid:\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Compute the sigmoid function\n",
        "        self.output = 1 / (1 + np.exp(-x))\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        # Use the stored output from the forward pass to compute the gradient\n",
        "        return grad_output * self.output * (1 - self.output)"
      ],
      "metadata": {
        "id": "phjy4NdwKa5L"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###C. Mean Squared Error Loss (MSELoss)"
      ],
      "metadata": {
        "id": "9vG68laSLHwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MSELoss:\n",
        "    def __call__(self, pred, target):\n",
        "        return self.forward(pred, target)\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        self.pred = pred  # Save for backward\n",
        "        self.target = target  # Save for backward\n",
        "        error = np.mean((pred - target) ** 2)  # Mean squared error\n",
        "        return error\n",
        "\n",
        "    def backward(self):\n",
        "        # Derivative of MSE w.r.t. predictions\n",
        "        return 2. * (self.pred - self.target) / self.target.shape[0]"
      ],
      "metadata": {
        "id": "aO9aHeBULJ4c"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Gradient Descent Optimizer\n"
      ],
      "metadata": {
        "id": "uN4Av6kNLWrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This SGD class performs parameter updates using gradients computed during backpropagation.\n",
        "class SGD:\n",
        "    def __init__(self, params, grads, lr=0.1):\n",
        "        self.params = params\n",
        "        self.grads = grads\n",
        "        self.lr = lr\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Update the parameters (weight and bias) of your netork using the Gradient Descent algorithm.\"\"\"\n",
        "        for p, g in zip(self.params, self.grads):\n",
        "            p -= self.lr * g # gradient descent\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"Zeros out the gradient of your network\"\"\"\n",
        "        for g in self.grads:\n",
        "            g.fill(0.0)"
      ],
      "metadata": {
        "id": "p04wWGycLdL6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Assemble the MLP\n"
      ],
      "metadata": {
        "id": "TKophSgFLoHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We have an MLP with:\n",
        "\n",
        "#     Input layer: 784 neurons (28×28)\n",
        "#     Hidden layer: 50 neurons with Sigmoid activation\n",
        "#     Output layer: 10 neurons (one per digit) with Sigmoid activation\n",
        "\n",
        "# What we have todo is:\n",
        "\n",
        "#     In __init__: Create two linear layers and two activations\n",
        "#     In forward: Pass input through layers in sequence\n",
        "#     e.g. x → linear1 → sigmoid → linear2 → sigmoid\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, in_features, hidden_size, out_features):\n",
        "        # Initialize layers and activations\n",
        "        self.linear1 = Linear(in_features, hidden_size)\n",
        "        self.act1 = Sigmoid()\n",
        "        self.linear2 = Linear(hidden_size, out_features)\n",
        "        self.act2 = Sigmoid()\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward propagation through your network\"\"\"\n",
        "        x = self.linear1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.act2(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"Backpropagation through your network\"\"\"\n",
        "        grad_z2 = self.act2.backward(grad_output)\n",
        "        grad_a1 = self.linear2.backward(grad_z2)\n",
        "\n",
        "        grad_z1 = self.act1.backward(grad_a1)\n",
        "        _ = self.linear1.backward(grad_z1)\n",
        "\n",
        "    def parameters(self):\n",
        "        \"\"\"Return the parameters of your entire model/network\"\"\"\n",
        "        return self.linear1.parameters() + self.linear2.parameters()\n",
        "\n",
        "    def gradients(self):\n",
        "        \"\"\"Return the gradients of your entire model/network\"\"\"\n",
        "        return self.linear1.gradients() + self.linear2.gradients()\n"
      ],
      "metadata": {
        "id": "aA87XIRwLuxR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. Training and Evaluation\n"
      ],
      "metadata": {
        "id": "7SWsj1kdMFX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###A. Training Function"
      ],
      "metadata": {
        "id": "rz-rBDcDMjRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Performs forward and backward passes for each mini-batch\n",
        "# Updates weights using SGD\n",
        "\n",
        "def train(model, train_loader, optimizer, criterion):\n",
        "    total_loss = 0.0\n",
        "    num_samples = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        preds = model(X_batch)\n",
        "        loss = criterion(preds, int_to_onehot(y_batch))\n",
        "\n",
        "        grad_output = loss_fn.backward()\n",
        "\n",
        "        model.backward(grad_output)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss\n",
        "        num_samples += len(y_batch)\n",
        "\n",
        "    return total_loss / num_samples"
      ],
      "metadata": {
        "id": "JBXOovePMtjA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###B. Validation Function"
      ],
      "metadata": {
        "id": "g5A77aSZMoDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computes loss and accuracy on the validation set\n",
        "def valid(model, valid_loader, criterion):\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    for X_batch, y_batch in valid_loader:\n",
        "        preds = model(X_batch)\n",
        "        loss = criterion(preds, int_to_onehot(y_batch))\n",
        "\n",
        "        total_loss += loss\n",
        "\n",
        "        predicted_labels = np.argmax(preds, axis=1)\n",
        "        correct_predictions += (predicted_labels == y_batch).sum()\n",
        "        total_predictions += len(y_batch)\n",
        "\n",
        "    total_loss /= total_predictions\n",
        "    accuracy = (correct_predictions / total_predictions)*100\n",
        "    return total_loss, accuracy"
      ],
      "metadata": {
        "id": "_cQo-IJZM-39"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###C. Convert Targets"
      ],
      "metadata": {
        "id": "Oo_SCnhlNR9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_onehot(y, num_labels=10):\n",
        "      return np.eye(num_labels)[y]"
      ],
      "metadata": {
        "id": "_6b-TXPjNWKd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8. Instantiate Model, Loss Function, and Optimizer"
      ],
      "metadata": {
        "id": "Yha-47zpNdlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before training, we must initialize our network and supporting components.\n",
        "in_features = 28 * 28      # Each image is 28x28 pixels\n",
        "hidden_size = 50           # Size of the hidden layer (can experiment)\n",
        "out_features = 10          # One output per digit class (0-9)\n",
        "learning_rate = 0.1        # Step size for SGD updates\n",
        "\n",
        "np.random.seed(123)        # For reproducible results\n",
        "\n",
        "model = MLP(in_features=in_features, hidden_size=hidden_size, out_features=out_features)\n",
        "loss_fn = MSELoss()\n",
        "optimizer = SGD(model.parameters(), model.gradients(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "_pKIhZkpNiDH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##9. Run the Training Loop\n"
      ],
      "metadata": {
        "id": "C2SsM9ceN1gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that everything is in place, we can train the MLP using mini-batch SGD:\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Usually in the PyTorch way, dataloader SHOULD NOT be in the epoch loop\n",
        "    train_loader = minibatch_generator(X_train, y_train, minibatch_size=batch_size)\n",
        "    valid_loader = minibatch_generator(X_valid, y_valid, minibatch_size=batch_size)\n",
        "\n",
        "    train_loss = train(model, train_loader, optimizer, criterion=loss_fn)\n",
        "    valid_loss, valid_acc = valid(model, valid_loader, criterion=loss_fn)\n",
        "\n",
        "    print(f\"Epoch: {epoch+1:02d}/{num_epochs} | Train MSE: {train_loss:.4f} | Valid MSE: {valid_loss:.4f} | Valid Acc: {valid_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRblyrEeN-Oc",
        "outputId": "7300cda6-51da-4981-b17b-8108427a5d34"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-13-766233039.py:8: RuntimeWarning: overflow encountered in exp\n",
            "  self.output = 1 / (1 + np.exp(-x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01/5 | Train MSE: 0.0002 | Valid MSE: 0.0002 | Valid Acc: 90.92%\n",
            "Epoch: 02/5 | Train MSE: 0.0002 | Valid MSE: 0.0001 | Valid Acc: 91.18%\n",
            "Epoch: 03/5 | Train MSE: 0.0001 | Valid MSE: 0.0001 | Valid Acc: 91.80%\n",
            "Epoch: 04/5 | Train MSE: 0.0001 | Valid MSE: 0.0001 | Valid Acc: 91.24%\n",
            "Epoch: 05/5 | Train MSE: 0.0001 | Valid MSE: 0.0001 | Valid Acc: 91.16%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10. Final Evaluation on Test Set"
      ],
      "metadata": {
        "id": "3RcrEWpQOghN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We are going to measure final generalization accuracy using:\n",
        "def test(model, test_loader):\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        preds = model(X_batch)\n",
        "\n",
        "        predicted_labels = np.argmax(preds, axis=1)\n",
        "        correct_predictions += (predicted_labels == y_batch).sum()\n",
        "        total_predictions += len(y_batch)\n",
        "\n",
        "    accuracy = (correct_predictions / total_predictions)*100\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "test_loader = minibatch_generator(X_test, y_test, minibatch_size=100)\n",
        "test_acc = test(model, test_loader)\n",
        "print(f'Test accuracy: {test_acc:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHNtJdIAOhwT",
        "outputId": "ea7131f5-35bd-4034-f44e-a93e7ae27ec1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-13-766233039.py:8: RuntimeWarning: overflow encountered in exp\n",
            "  self.output = 1 / (1 + np.exp(-x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 90.62%\n"
          ]
        }
      ]
    }
  ]
}